{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Stroke Data Benchmark Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Import libraries\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.metrics import classification_report, confusion_matrix\r\n",
    "from sklearn.model_selection import GridSearchCV"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Read in preprocessed data\r\n",
    "x_train = pd.read_csv('data/x_train.csv')\r\n",
    "x_test = pd.read_csv('data/x_test.csv')\r\n",
    "y_train = pd.read_csv('data/y_train.csv')\r\n",
    "y_test = pd.read_csv('data/y_test.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Benchmark Model\n",
    "\n",
    "### Linear Regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "model = LogisticRegression(solver='liblinear', random_state=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear Regression\r\n",
    "\r\n",
    "A simple binary classifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Instantiate model\r\n",
    "model = LogisticRegression(solver='liblinear', random_state=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Train model\r\n",
    "model.fit(x_train, y_train.values.ravel())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0, solver='liblinear')"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Test model\r\n",
    "model.predict(x_test)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Calculate F1 score"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "matrix = confusion_matrix(y_test, model.predict(x_test))\r\n",
    "print(matrix)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1589    2]\n [  95    1]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "recall = matrix[1][1] / (matrix[1][1] + matrix[1][0])\r\n",
    "print('Recall score: ' + str(round(recall, 4)))\r\n",
    "\r\n",
    "precision = matrix[1][1] / (matrix[1][1] + matrix[0][1])\r\n",
    "print('Precision score: ' + str(round(precision, 4)))\r\n",
    "\r\n",
    "f1 = (2 * precision * recall / (precision + recall))\r\n",
    "print('F1 score: ' + str(round(f1, 4)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Recall score: 0.0104\nPrecision score: 0.3333\nF1 score: 0.0202\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tweak model parameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "model = LogisticRegression(solver='liblinear', class_weight='balanced', random_state=0)\r\n",
    "model.fit(x_train, y_train.values.ravel())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='balanced', random_state=0, solver='liblinear')"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "matrix = confusion_matrix(y_test, model.predict(x_test))\r\n",
    "print(matrix)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1178  413]\n [  24   72]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "recall = matrix[1][1] / (matrix[1][1] + matrix[1][0])\r\n",
    "print('Recall score: ' + str(round(recall, 4)))\r\n",
    "\r\n",
    "precision = matrix[1][1] / (matrix[1][1] + matrix[0][1])\r\n",
    "print('Precision score: ' + str(round(precision, 4)))\r\n",
    "\r\n",
    "f1 = (2 * precision * recall / (precision + recall))\r\n",
    "print('F1 score: ' + str(round(f1, 4)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Recall score: 0.75\n",
      "Precision score: 0.1485\n",
      "F1 score: 0.2478\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "model.get_params()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': 'balanced',\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'l1_ratio': None,\n",
       " 'max_iter': 100,\n",
       " 'multi_class': 'auto',\n",
       " 'n_jobs': None,\n",
       " 'penalty': 'l2',\n",
       " 'random_state': 0,\n",
       " 'solver': 'liblinear',\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using Grid Search"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "lr = LogisticRegression()             # initialize the model\r\n",
    "\r\n",
    "solver_options = ['liblinear']\r\n",
    "multi_class_options = ['ovr']\r\n",
    "#class_weight_options = ['balanced']\r\n",
    "\r\n",
    "#Setting the range for class weights\r\n",
    "weights = np.linspace(0.0,0.99,20)\r\n",
    "\r\n",
    "#Creating a dictionary grid for grid search\r\n",
    "class_weight_options = [{0:x, 1:1.0-x} for x in weights]\r\n",
    "\r\n",
    "max_iter_value = [1000000]\r\n",
    "c_values = [1, 10, 100, 1000]\r\n",
    "\r\n",
    "param_grid = dict(solver = solver_options, multi_class = multi_class_options, class_weight = class_weight_options, max_iter = max_iter_value, C = c_values)\r\n",
    "\r\n",
    "grid = GridSearchCV(lr, param_grid, scoring = 'f1', n_jobs=-1)\r\n",
    "grid.fit(x_train, y_train.values.ravel())"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=LogisticRegression(), n_jobs=-1,\n",
       "             param_grid={'C': [1, 10, 100, 1000],\n",
       "                         'class_weight': [{0: 0.0, 1: 1.0},\n",
       "                                          {0: 0.05210526315789474,\n",
       "                                           1: 0.9478947368421052},\n",
       "                                          {0: 0.10421052631578948,\n",
       "                                           1: 0.8957894736842105},\n",
       "                                          {0: 0.1563157894736842,\n",
       "                                           1: 0.8436842105263158},\n",
       "                                          {0: 0.20842105263157895,\n",
       "                                           1: 0.791578947368421},\n",
       "                                          {0: 0.2605263157894737,\n",
       "                                           1: 0.7394736842105263},\n",
       "                                          {0: 0.3126315...\n",
       "                                           1: 0.3226315789473684},\n",
       "                                          {0: 0.7294736842105263,\n",
       "                                           1: 0.2705263157894737},\n",
       "                                          {0: 0.781578947368421,\n",
       "                                           1: 0.21842105263157896},\n",
       "                                          {0: 0.8336842105263158,\n",
       "                                           1: 0.1663157894736842},\n",
       "                                          {0: 0.8857894736842106,\n",
       "                                           1: 0.11421052631578943},\n",
       "                                          {0: 0.9378947368421053,\n",
       "                                           1: 0.06210526315789466},\n",
       "                                          {0: 0.99, 1: 0.010000000000000009}],\n",
       "                         'max_iter': [1000000], 'multi_class': ['ovr'],\n",
       "                         'solver': ['liblinear']},\n",
       "             scoring='f1')"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "grid.best_estimator_"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression(C=1,\n",
       "                   class_weight={0: 0.10421052631578948, 1: 0.8957894736842105},\n",
       "                   max_iter=1000000, multi_class='ovr', solver='liblinear')"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "matrix = confusion_matrix(y_test, grid.best_estimator_.predict(x_test))\r\n",
    "print(matrix)\r\n",
    "\r\n",
    "recall = matrix[1][1] / (matrix[1][1] + matrix[1][0])\r\n",
    "print('Recall score: ' + str(round(recall, 4)))\r\n",
    "\r\n",
    "precision = matrix[1][1] / (matrix[1][1] + matrix[0][1])\r\n",
    "print('Precision score: ' + str(round(precision, 4)))\r\n",
    "\r\n",
    "f1 = (2 * precision * recall / (precision + recall))\r\n",
    "print('F1 score: ' + str(round(f1, 4)))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1413  178]\n [  48   48]]\nRecall score: 0.5\nPrecision score: 0.2124\nF1 score: 0.2981\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "interpreter": {
   "hash": "b03aef1243c5a0cd4191fb1d766d2cdc446a8616d32b2f066886d2fdf0c1db8c"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}